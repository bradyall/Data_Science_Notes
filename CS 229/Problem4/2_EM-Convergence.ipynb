{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229: Problem Set 4\n",
    "## Problem 2: Expectation-Maximization Convergence \n",
    "\n",
    "\n",
    "**C. Combier**\n",
    "\n",
    "This iPython Notebook provides solutions to Stanford's CS229 (Machine Learning, Fall 2017) graduate course problem set 3, taught by Andrew Ng.\n",
    "\n",
    "The problem set can be found here: [./ps4.pdf](ps4.pdf)\n",
    "\n",
    "I chose to write the solutions to the coding questions in Python, whereas the Stanford class is taught with Matlab/Octave.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x_i$ is the $i^{th}$ feature vector\n",
    "- $y_i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $z_i$'s are the latent (hidden) variables\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2URlArNNdz_q"
   },
   "source": [
    "After the E-step, we obtain a lower bound on the log-likelihood denoted by:\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_i^m \\sum_{z_i} Q_i(z_i) \\log \\frac{ p(x_i, z_i;\\theta)}{Q_i (z_i)}\n",
    "$$\n",
    "\n",
    "This lower bound $\\beta$ has been made tight by setting:\n",
    "\n",
    "$$\n",
    "Q_i(z_i) = p(z_i |x_i; \\theta) = \\frac{p(x_i, z_i; \\theta)}{p(x_i; \\theta)}\n",
    "$$\n",
    "\n",
    "For the M-step, we maximize $\\beta$ by taking the gradient with respect to $\\theta$ and setting it to zero.\n",
    "\n",
    "Suppose that EM hase converged, and that $\\theta = \\theta^*$.\n",
    "\n",
    "In this case:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\beta & = \\sum_i^m \\sum_{z_i} Q_i(z_i) \\nabla_{\\theta}\\log \\frac{ p(x_i, z_i;\\theta)}{Q_i (z_i)}_{| \\theta = \\theta^*} \\\\\n",
    "&= \\sum_i^m \\sum_{z_i} Q_i(z_i)  \\frac{Q_i(z_i) }{p(x_i, z_i;\\theta^*) Q_i (z_i)} \\nabla_{\\theta}  p(x_i, z_i;\\theta)_{| \\theta = \\theta^*}  \\\\\n",
    "&= \\sum_i^m \\sum_{z_i}  \\frac{p(x_i, z_i; \\theta^*)}{p(x_i; \\theta^*) p(x_i, z_i; \\theta^*)} \\nabla_{\\theta}  p(x_i, z_i;\\theta^*)_{| \\theta = \\theta^*} \\\\\n",
    "&= \\sum_i^m \\sum_{z_i}  \\frac{\\nabla_{\\theta}  p(x_i, z_i;\\theta)_{| \\theta = \\theta^*}  }{p(x_i; \\theta^*) } \\\\\n",
    "&= \\sum_i^m \\frac{\\nabla_{\\theta}  p(x_i;\\theta)_{| \\theta = \\theta^*}  }{p(x_i; \\theta^*) } \\\\\n",
    "&= \\sum_i^m \\nabla_{\\theta}  \\log p(x_i;\\theta)_{| \\theta = \\theta^*}   \\\\\n",
    "&= \\nabla_{\\theta} ( \\sum_i^m  \\log p(x_i;\\theta) )_{| \\theta = \\theta^*}\\\\\n",
    "&= \\nabla_{\\theta} \\ell (\\theta)_{| \\theta = \\theta^*}\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bonjour, Colaboratory",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
