{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.],\n       [0.],\n       [0.]])"
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_fwf('Data/logistic_x.txt', header=None)\n",
    "y = pd.read_fwf('Data/logistic_y.txt', header=None)\n",
    "\n",
    "X[2] = 1\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([41.35229232]),\n array([36.94667679]),\n array([33.80560234]),\n array([31.55975723]),\n array([29.9479718]),\n array([28.78537391]),\n array([27.94109771]),\n array([27.32250518]),\n array([26.86401745]),\n array([26.51920925]),\n array([26.25521307]),\n array([26.04875814]),\n array([25.88336678]),\n array([25.74736977]),\n array([25.63250163]),\n array([25.53290624]),\n array([25.44443301]),\n array([25.36413871]),\n array([25.28993478]),\n array([25.22033773]),\n array([25.15429245]),\n array([25.09104716]),\n array([25.03006482]),\n array([24.97096052]),\n array([24.9134571]),\n array([24.85735376]),\n array([24.8025038]),\n array([24.74879892]),\n array([24.69615807]),\n array([24.64451955]),\n array([24.59383546]),\n array([24.54406771]),\n array([24.49518523]),\n array([24.44716202]),\n array([24.39997569]),\n array([24.3536065]),\n array([24.30803665]),\n array([24.26324979]),\n array([24.21923062]),\n array([24.1759647]),\n array([24.13343822]),\n array([24.09163789]),\n array([24.05055086]),\n array([24.01016462]),\n array([23.970467]),\n array([23.93144609]),\n array([23.89309024]),\n array([23.85538803]),\n array([23.81832826]),\n array([23.78189995]),\n array([23.74609231]),\n array([23.71089475]),\n array([23.67629685]),\n array([23.64228838]),\n array([23.60885931]),\n array([23.57599975]),\n array([23.54369999]),\n array([23.5119505]),\n array([23.4807419]),\n array([23.45006498]),\n array([23.41991067]),\n array([23.39027008]),\n array([23.36113444]),\n array([23.33249516]),\n array([23.30434378]),\n array([23.27667199]),\n array([23.24947162]),\n array([23.22273464]),\n array([23.19645315]),\n array([23.17061939]),\n array([23.14522574]),\n array([23.12026469]),\n array([23.09572888]),\n array([23.07161107]),\n array([23.04790413]),\n array([23.02460106]),\n array([23.00169498]),\n array([22.97917913]),\n array([22.95704686]),\n array([22.93529164]),\n array([22.91390704]),\n array([22.89288676]),\n array([22.87222457]),\n array([22.85191439]),\n array([22.83195021]),\n array([22.81232614]),\n array([22.79303639]),\n array([22.77407526]),\n array([22.75543715]),\n array([22.73711656]),\n array([22.71910808]),\n array([22.7014064]),\n array([22.68400628]),\n array([22.6669026]),\n array([22.65009029]),\n array([22.63356439]),\n array([22.61732004]),\n array([22.60135242]),\n array([22.58565683]),\n array([22.57022862])]"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "\n",
    "def cost_function(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y)\n",
    "    loss = 0\n",
    "    ## Calculate the cost with the given parameters\n",
    "    hypothesis = X.dot(beta)\n",
    "    for i in range(X.shape[0]):\n",
    "        loss += (hypothesis[i]-y[i])**2\n",
    "        J = 1 / 2 * loss\n",
    "\n",
    "    return J\n",
    "\n",
    "cost_function(X, y, thetas)\n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent() performs gradient descent to learn beta by\n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        hypothesis = X.dot(beta)\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                gradient += (hypothesis[i]-y[i])*X[i,j]\n",
    "            beta[j] = beta[j] - alpha*gradient\n",
    "        cost = cost_function(X, y, beta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "        ## If you really want to merge everything in one line:\n",
    "        # beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)\n",
    "        # cost = cost_function(X, y, beta)\n",
    "        # cost_history[iteration] = cost\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "alpha = 0.0005\n",
    "(b, c) = gradient_descent(X, y, thetas, alpha, 100)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastic Gradient Descent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(thetas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[41.35229232]]),\n array([[36.94667679]]),\n array([[33.80560234]]),\n array([[31.55975723]]),\n array([[29.9479718]]),\n array([[28.78537391]]),\n array([[27.94109771]]),\n array([[27.32250518]]),\n array([[26.86401745]]),\n array([[26.51920925]]),\n array([[26.25521307]]),\n array([[26.04875814]]),\n array([[25.88336678]]),\n array([[25.74736977]]),\n array([[25.63250163]]),\n array([[25.53290624]]),\n array([[25.44443301]]),\n array([[25.36413871]]),\n array([[25.28993478]]),\n array([[25.22033773]]),\n array([[25.15429245]]),\n array([[25.09104716]]),\n array([[25.03006482]]),\n array([[24.97096052]]),\n array([[24.9134571]]),\n array([[24.85735376]]),\n array([[24.8025038]]),\n array([[24.74879892]]),\n array([[24.69615807]]),\n array([[24.64451955]]),\n array([[24.59383546]]),\n array([[24.54406771]]),\n array([[24.49518523]]),\n array([[24.44716202]]),\n array([[24.39997569]]),\n array([[24.3536065]]),\n array([[24.30803665]]),\n array([[24.26324979]]),\n array([[24.21923062]]),\n array([[24.1759647]]),\n array([[24.13343822]]),\n array([[24.09163789]]),\n array([[24.05055086]]),\n array([[24.01016462]]),\n array([[23.970467]]),\n array([[23.93144609]]),\n array([[23.89309024]]),\n array([[23.85538803]]),\n array([[23.81832826]]),\n array([[23.78189995]]),\n array([[23.74609231]]),\n array([[23.71089475]]),\n array([[23.67629685]]),\n array([[23.64228838]]),\n array([[23.60885931]]),\n array([[23.57599975]]),\n array([[23.54369999]]),\n array([[23.5119505]]),\n array([[23.4807419]]),\n array([[23.45006498]]),\n array([[23.41991067]]),\n array([[23.39027008]]),\n array([[23.36113444]]),\n array([[23.33249516]]),\n array([[23.30434378]]),\n array([[23.27667199]]),\n array([[23.24947162]]),\n array([[23.22273464]]),\n array([[23.19645315]]),\n array([[23.17061939]]),\n array([[23.14522574]]),\n array([[23.12026469]]),\n array([[23.09572888]]),\n array([[23.07161107]]),\n array([[23.04790413]]),\n array([[23.02460106]]),\n array([[23.00169498]]),\n array([[22.97917913]]),\n array([[22.95704686]]),\n array([[22.93529164]]),\n array([[22.91390704]]),\n array([[22.89288676]]),\n array([[22.87222457]]),\n array([[22.85191439]]),\n array([[22.83195021]]),\n array([[22.81232614]]),\n array([[22.79303639]]),\n array([[22.77407526]]),\n array([[22.75543715]]),\n array([[22.73711656]]),\n array([[22.71910808]]),\n array([[22.7014064]]),\n array([[22.68400628]]),\n array([[22.6669026]]),\n array([[22.65009029]]),\n array([[22.63356439]]),\n array([[22.61732004]]),\n array([[22.60135242]]),\n array([[22.58565683]]),\n array([[22.57022862]])]"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "\n",
    "def cost_function(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y)\n",
    "\n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = 1/2*(X.dot(beta)-y).T.dot((X.dot(beta)-y))\n",
    "\n",
    "    return J\n",
    "\n",
    "cost_function(X, y, thetas)\n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent() performs gradient descent to learn beta by\n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        hypothesis = X.dot(beta)\n",
    "        loss = hypothesis-y\n",
    "        gradient = X.T.dot(loss)\n",
    "        beta = beta - alpha*gradient\n",
    "        cost = cost_function(X, y, beta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "        ## If you really want to merge everything in one line:\n",
    "        # beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)\n",
    "        # cost = cost_function(X, y, beta)\n",
    "        # cost_history[iteration] = cost\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "alpha = 0.0005\n",
    "(b, c) = gradient_descent(X, y, thetas, alpha, 100)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-4.94325123],\n       [ 6.35832332],\n       [-2.525     ]])"
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "\n",
    "def cost_function(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y)\n",
    "    hypothesis = 1/(1+math.e**(-(X.dot(beta))))\n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = 1/2*(hypothesis-y).T.dot((hypothesis-y))\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent() performs gradient descent to learn beta by\n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "    hypothesis = 1/(1+math.e**(-(X.dot(beta))))\n",
    "    for iteration in range(iterations):\n",
    "        loss = hypothesis-y\n",
    "        gradient = X.T.dot(loss)\n",
    "        beta = beta - alpha*gradient\n",
    "        cost = cost_function(X, y, beta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "        ## If you really want to merge everything in one line:\n",
    "        # beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)\n",
    "        # cost = cost_function(X, y, beta)\n",
    "        # cost_history[iteration] = cost\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "alpha = 0.0005\n",
    "(b, c) = gradient_descent(X, y, thetas, alpha, 100)\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4t/jqgtn_7128l6cw1_29rck3t40000gn/T/ipykernel_40346/3680592327.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1/(1-math.e**(-theta.T.dot(x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[             inf,  2.02369289e-320,  2.24336921e-314, ...,\n         9.42795298e-264,  2.40307842e-306,  8.06648694e-308],\n       [             nan,              inf,              nan, ...,\n                     nan,              nan,  1.79186525e-298],\n       [             nan,              nan,              inf, ...,\n                     nan,  2.16517159e-299,  1.23983118e-302],\n       ...,\n       [ 6.44533354e-311,  1.59761465e-279,  1.53726994e-311, ...,\n                     inf,  3.99546069e+252,  7.34295013e+223],\n       [ 2.42368889e+188,  6.93890661e+218,  1.33360360e+241, ...,\n         2.76217432e+257,              inf,  2.10992986e-312],\n       [ 0.00000000e+000,  4.88907831e-311,  8.80117766e-282, ...,\n        -8.25259115e+252,  7.84019370e-061,              inf]])"
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "\n",
    "def sigmoid(x, theta):\n",
    "    return 1/(1-math.e**(-theta.T.dot(x)))\n",
    "def hessian(x):\n",
    "    \"\"\"\n",
    "    Calculate the hessian matrix with finite differences\n",
    "    Parameters:\n",
    "       - x : ndarray\n",
    "    Returns:\n",
    "       an array of shape (x.dim, x.ndim) + x.shape\n",
    "       where the array[i, j, ...] corresponds to the second derivative x_ij\n",
    "    \"\"\"\n",
    "    diag = np.empty(x.shape[0])\n",
    "    hessian = np.empty([x.shape[0], x.shape[0]])\n",
    "    for i in range(x.shape[0]):\n",
    "        diag[i] = sigmoid(x[i,:], thetas).dot(sigmoid(-x[i,:], thetas))\n",
    "        hessian[i,i] = (x[i,:].dot(x[i,:].T))*(diag[i])\n",
    "\n",
    "\n",
    "    return hessian\n",
    "\n",
    "hessian(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.6716252 , -0.66557395,  0.5       ],\n       [ 0.91027645, -0.31733405,  0.5       ],\n       [ 0.49316034, -0.9442881 ,  0.5       ],\n       [ 0.9721867 , -0.817726  ,  0.5       ],\n       [ 0.48836676, -0.67665755,  0.5       ],\n       [ 0.9729292 , -1.0221639 ,  0.5       ],\n       [ 1.05375765, -1.0628342 ,  0.5       ],\n       [ 1.0351865 , -1.23170505,  0.5       ],\n       [ 0.43432482, -1.2059674 ,  0.5       ],\n       [ 0.9003297 , -1.38698445,  0.5       ],\n       [ 1.56418935, -1.7226216 ,  0.5       ],\n       [ 1.54737145, -1.82230725,  0.5       ],\n       [ 1.4543326 , -2.00325185,  0.5       ],\n       [ 1.3385169 , -1.5099296 ,  0.5       ],\n       [ 1.37293355, -1.35502805,  0.5       ],\n       [ 2.08573235, -1.7311241 ,  0.5       ],\n       [ 1.965661  , -1.0549522 ,  0.5       ],\n       [ 2.1893435 , -1.19023715,  0.5       ],\n       [ 2.40082825, -1.6901672 ,  0.5       ],\n       [ 2.0830525 , -1.4069422 ,  0.5       ],\n       [ 1.23350705, -0.8054222 ,  0.5       ],\n       [ 1.74133715, -0.7766936 ,  0.5       ],\n       [ 1.6826241 , -0.9082468 ,  0.5       ],\n       [ 1.4386394 , -0.92558445,  0.5       ],\n       [ 1.5545222 , -0.8192473 ,  0.5       ],\n       [ 1.10918505,  0.03713978,  0.5       ],\n       [ 0.99749365,  0.08134329,  0.5       ],\n       [ 1.4750154 ,  0.00843651,  0.5       ],\n       [ 1.01080045,  0.08613693,  0.5       ],\n       [ 1.02434605, -0.31790521,  0.5       ],\n       [ 0.43774282, -0.27293084,  0.5       ],\n       [ 0.28539971, -0.01663933,  0.5       ],\n       [ 0.7133234 , -0.37644169,  0.5       ],\n       [ 0.36132816, -0.43345965,  0.5       ],\n       [ 0.47673099, -0.7448478 ,  0.5       ],\n       [ 2.41666665,  0.03508772,  0.5       ],\n       [ 2.15350875,  0.70760235,  0.5       ],\n       [ 3.01608185,  0.2251462 ,  0.5       ],\n       [ 2.70906435, -1.35380115,  0.5       ],\n       [ 1.72953215, -1.4122807 ,  0.5       ],\n       [ 1.3640351 , -0.4619883 ,  0.5       ],\n       [ 0.501462  ,  0.38596491,  0.5       ],\n       [ 1.83187135, -0.38888889,  0.5       ],\n       [ 2.15350875, -0.52046785,  0.5       ],\n       [ 1.84649125, -0.05263158,  0.5       ],\n       [ 2.86988305, -0.8128655 ,  0.5       ],\n       [ 2.4897661 , -0.75438595,  0.5       ],\n       [ 3.25      , -1.45614035,  0.5       ],\n       [ 2.62134505,  0.45906432,  0.5       ],\n       [ 0.8377193 ,  0.28362573,  0.5       ],\n       [-2.58544985, -0.60518335, -0.5       ],\n       [-2.4397594 , -0.8040924 , -0.5       ],\n       [-2.3324935 , -0.5347766 , -0.5       ],\n       [-2.24671605, -0.6175796 , -0.5       ],\n       [-2.07564835, -0.4336063 , -0.5       ],\n       [-1.858854  , -0.57586   , -0.5       ],\n       [-1.81122385, -0.65533845, -0.5       ],\n       [-1.53034715, -0.74285815, -0.5       ],\n       [-3.53592325,  0.17480825, -0.5       ],\n       [-3.0195916 ,  0.12378416, -0.5       ],\n       [-3.337374  ,  0.06242383, -0.5       ],\n       [-3.42306455, -0.12988584, -0.5       ],\n       [-3.2135362 ,  0.07356931, -0.5       ],\n       [-3.42280325, -0.73774835, -0.5       ],\n       [-3.8527003 , -0.80227775, -0.5       ],\n       [-3.1435329 , -1.20782135, -0.5       ],\n       [-3.4905478 , -0.62999325, -0.5       ],\n       [-3.5495086 , -1.10775755, -0.5       ],\n       [-2.76377395, -0.14984211, -0.5       ],\n       [-2.91517445,  0.10987204, -0.5       ],\n       [-3.17972635, -0.11972109, -0.5       ],\n       [-3.0502262 ,  0.02047871, -0.5       ],\n       [-2.8118706 , -0.18567957, -0.5       ],\n       [-2.94184845, -1.3884093 , -0.5       ],\n       [-2.78908055, -1.53414445, -0.5       ],\n       [-3.5025331 ,  0.12890863, -0.5       ],\n       [-2.2269057 , -0.41970916, -0.5       ],\n       [-2.8247962 , -0.65269645, -0.5       ],\n       [-2.31687445, -0.9733773 , -0.5       ],\n       [-1.84934235, -1.1297042 , -0.5       ],\n       [-2.05965025, -1.2737255 , -0.5       ],\n       [-2.3832779 , -1.37656045, -0.5       ],\n       [-1.5406049 , -1.39926275, -0.5       ],\n       [-2.0365497 ,  1.5146199 , -0.5       ],\n       [-1.74415205,  0.94444445, -0.5       ],\n       [-0.38450292, -0.60526315, -0.5       ],\n       [-0.75      , -1.90643275, -0.5       ],\n       [-2.8991228 ,  1.04678365, -0.5       ],\n       [-3.40572645,  0.41728365, -0.5       ],\n       [-3.5553048 ,  0.5100579 , -0.5       ],\n       [-3.747076  ,  0.87134505, -0.5       ],\n       [-1.56871345, -0.21052631, -0.5       ],\n       [-0.8377193 , -0.25438596, -0.5       ],\n       [-1.247076  ,  0.43274854, -0.5       ],\n       [-2.3874269 , -0.0497076 , -0.5       ],\n       [-2.9137427 ,  0.34502924, -0.5       ],\n       [-1.14473685, -0.9853801 , -0.5       ],\n       [-1.247076  , -0.70760235, -0.5       ],\n       [-1.04239765, -0.67836255, -0.5       ]])"
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "def sigmoid(func):\n",
    "    return 1/(1+np.exp(-func))\n",
    "\n",
    "def grad(x, y, beta):\n",
    "    z = y*x.dot(beta)\n",
    "    return np.array((1-sigmoid(z))*-y*x)\n",
    "\n",
    "def hessian(x, y, beta):\n",
    "\n",
    "    hessian = np.zeros([X.shape[1], X.shape[1]])\n",
    "    z = y*x.dot(beta)\n",
    "\n",
    "    for j in range(X.shape[1]):\n",
    "        for i in range(X.shape[1]):\n",
    "            hessian[i,j] = np.mean(sigmoid(z)*(1-sigmoid(z))*x[:,i]*x[:,j])\n",
    "    return hessian\n",
    "\n",
    "def hess_l(theta, x, y):\n",
    "    hess = np.zeros((x.shape[1], x.shape[1]))\n",
    "    z = y*x.dot(theta)\n",
    "    for i in range(hess.shape[0]):\n",
    "        for j in range(hess.shape[0]):\n",
    "            if i <= j:\n",
    "                hess[i][j] = np.mean(sigmoid(z)*(1-sigmoid(z))*x[:,i]*x[:,j])\n",
    "                if i != j:\n",
    "                    hess[j][i] = hess[i][j]\n",
    "    return hess\n",
    "\n",
    "\n",
    "# hess_l(thetas, X, y)\n",
    "grad(X, y, thetas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Newton's Method"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (99,99) (99,3) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [353]\u001B[0m, in \u001B[0;36m<cell line: 39>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;66;03m## If you really want to merge everything in one line:\u001B[39;00m\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;66;03m# beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)\u001B[39;00m\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;66;03m# cost = cost_function(X, y, beta)\u001B[39;00m\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;66;03m# cost_history[iteration] = cost\u001B[39;00m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m beta, cost_history\n\u001B[0;32m---> 39\u001B[0m (b, c) \u001B[38;5;241m=\u001B[39m \u001B[43mnewtons_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthetas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m b\n",
      "Input \u001B[0;32mIn [353]\u001B[0m, in \u001B[0;36mnewtons_method\u001B[0;34m(x, y, beta, iterations)\u001B[0m\n\u001B[1;32m     24\u001B[0m hypothesis \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m*\u001B[39mx\u001B[38;5;241m.\u001B[39mdot(beta)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m iteration \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(iterations):\n\u001B[0;32m---> 26\u001B[0m     gradient \u001B[38;5;241m=\u001B[39m \u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     inv_hessian \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39minv(hessian(x, y, beta))\n\u001B[1;32m     28\u001B[0m     beta \u001B[38;5;241m=\u001B[39m beta \u001B[38;5;241m-\u001B[39m inv_hessian\u001B[38;5;241m.\u001B[39mdot(gradient\u001B[38;5;241m.\u001B[39mT)\n",
      "Input \u001B[0;32mIn [350]\u001B[0m, in \u001B[0;36mgrad\u001B[0;34m(x, y, beta)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrad\u001B[39m(x, y, beta):\n\u001B[1;32m      7\u001B[0m     z \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m*\u001B[39mx\u001B[38;5;241m.\u001B[39mdot(beta)\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43my\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m)\n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (99,99) (99,3) "
     ]
    }
   ],
   "source": [
    "thetas = np.zeros(X.shape[1]).reshape((3,1))\n",
    "thetas\n",
    "\n",
    "def cost_function(x, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y)\n",
    "    hypothesis = y*x.dot(beta)\n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.log(1 + np.exp(-hypothesis))\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def newtons_method(x, y, beta, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent() performs gradient descent to learn beta by\n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "    hypothesis = y*x.dot(beta)\n",
    "    for iteration in range(iterations):\n",
    "        gradient = grad(x, y, beta)\n",
    "        inv_hessian = np.linalg.inv(hessian(x, y, beta))\n",
    "        beta = beta - inv_hessian.dot(gradient.T)\n",
    "        cost = cost_function(X, y, beta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "        ## If you really want to merge everything in one line:\n",
    "        # beta = beta - alpha * (X.T.dot(X.dot(beta)-y)/m)\n",
    "        # cost = cost_function(X, y, beta)\n",
    "        # cost_history[iteration] = cost\n",
    "\n",
    "    return beta, cost_history\n",
    "\n",
    "(b, c) = newtons_method(X, y, thetas, 100)\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
