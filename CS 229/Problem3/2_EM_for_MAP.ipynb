{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229: Problem Set 3\n",
    "## Problem 2: Expectation-Maximization for Maximum a Posteriori\n",
    "\n",
    "\n",
    "**C. Combier**\n",
    "\n",
    "This iPython Notebook provides solutions to Stanford's CS229 (Machine Learning, Fall 2017) graduate course problem set 3, taught by Andrew Ng.\n",
    "\n",
    "The problem set can be found here: [./ps3.pdf](ps3.pdf)\n",
    "\n",
    "I chose to write the solutions to the coding questions in Python, whereas the Stanford class is taught with Matlab/Octave.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $x^i$ is the $i^{th}$ feature vector\n",
    "- $y^i$ is the expected outcome for the $i^{th}$ training example\n",
    "- $m$ is the number of training examples\n",
    "- $n$ is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is very similar to the derivation of the EM algorithm for MLE given in the lectures notes. The difference is that we are now in a Bayesian setting, and impose a prior on $\\theta$:\n",
    "\n",
    "$$\n",
    "MAP = \\prod_i^m \\sum_{z^i} p(x^i, z^i | \\theta)p(\\theta)\n",
    "$$\n",
    "\n",
    "Here, $z^i$ denotes the latent (hidden) random variables.\n",
    "\n",
    "### Step 1: E-step\n",
    "\n",
    "1. We start by taking the log-MAP:\n",
    "\n",
    "$$\n",
    "\\log MAP = \\sum_i^m \\log \\sum_{z^i} Q_i(z^i) \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta)\n",
    "$$\n",
    "\n",
    "2. We apply Jensen's inequality to the above formula:\n",
    "\n",
    "$$\n",
    "\\log MAP \\geq \\sum_i^m  \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta)\n",
    "$$\n",
    "\n",
    "3. Next, we need to choose a distribution $Q_i$ for $z^i$. The above inequality become an equality if $\\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} = cste$, which will lead to the inequality becoming tight for the current value of $\\theta$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} = \\lambda & \\iff Q_i(z^i) = \\frac{1}{\\lambda} p(x^i, z^i | \\theta) \\\\\n",
    "& \\iff Q_i(z^i) = \\frac{p(x^i, z^i | \\theta) }{\\sum_{z^i}p(x^i, z^i | \\theta)} \\\\\n",
    "& \\iff Q_i(z^i) = \\frac{p(x^i, z^i | \\theta) }{p(x^i | \\theta)} \\\\\n",
    "& \\iff Q_i(z^i) = p(z^i | x^i, \\theta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This obtained by using the fact that since $Q_i$ is a distribution, $\\sum_{z^i} Q_i(z^i) = 1 \\implies \\lambda = \\sum_{z^i} p(x^i,z^i | \\theta)$.\n",
    "\n",
    "**This completes the E-step of the EM algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: M-step\n",
    "\n",
    "For the M-step, we simply maximize the expression obtained in step 2) with respect to $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta := \\text{arg}\\max_{\\theta} \\sum_i^m  \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta)\n",
    "$$\n",
    "\n",
    "As usual, we do this by taking the gradient with respect to $\\theta$ and setting it to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Convergence\n",
    "\n",
    "We consider two successive iterations $k+1$ and $k$  of EM, and we will prove that $\\ell(\\theta^{k+1}) \\geq \\ell(\\theta^k)$, i.e. that $\\ell$ is monotonically increasing.\n",
    "\n",
    "We refer the reader to the lecture notes, as the proof is the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
